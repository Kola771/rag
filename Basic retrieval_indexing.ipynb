{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BASIC RAG - Indexing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ingestion\n",
    "\n",
    "On commence par charger les documents :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import SimpleDirectoryReader, Document\n",
    "\n",
    "documents = SimpleDirectoryReader(\"data\", recursive = True).load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "for doc in documents : \n",
    "    doc.text = doc.text.replace(\"’\", \"'\")\n",
    "    # doc.text = re.sub(r'[a-zA-Z]\\n[a-z]', '', doc.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ChromaDB persistent client\n",
    "\n",
    "On cherche ensuite à les indexer dans un client persistent ChromaDb en précisant le modèle d'embedding.  \n",
    "Ici, on a pris le modèle HuggingFace sentence-transformers/all-MiniLm-L6-v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "\n",
    "sentence_transformer_ef = embedding_functions.SentenceTransformerEmbeddingFunction(model_name=\"all-MiniLM-L6-v2\")\n",
    "with open(\"log/embedding_function.txt\", 'w') as file : \n",
    "    file.write(\"embedding_functions.SentenceTransformerEmbeddingFunction(model_name='all-MiniLM-L6-v2')\")\n",
    "chroma_client = chromadb.PersistentClient(\"./dbs/documentation/chroma_index\")\n",
    "try : \n",
    "    chroma_collection = chroma_client.create_collection(\"Basic_rag\", embedding_function=sentence_transformer_ef)\n",
    "except : \n",
    "    chroma_client.delete_collection(name=\"Basic_rag\")\n",
    "    chroma_collection = chroma_client.create_collection(\"Basic_rag\", embedding_function=sentence_transformer_ef)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding \n",
    "\n",
    "- LangChain Integrations\n",
    "\n",
    "Link : https://docs.llamaindex.ai/en/stable/examples/embeddings/huggingface.html\n",
    "\n",
    "On vient charger le modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modèle \n",
    "\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from llama_index.embeddings import LangchainEmbedding\n",
    "\n",
    "embed_model = LangchainEmbedding(HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On choisit ensuite les paramètres du découpages des chunks : leur taille et l'overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text splitter \n",
    "\n",
    "from llama_index.text_splitter import SentenceSplitter\n",
    "\n",
    "text_splitter = SentenceSplitter(chunk_size=200, chunk_overlap=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Puis, on peut lancer l'embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original size collection :  0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e4a3728c0c34c729f1f9960a7dff4d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parsing nodes:   0%|          | 0/139 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84fed0cdd64d4e729a5b53707d2b27e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/979 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Post-embeddind size collection :  979\n"
     ]
    }
   ],
   "source": [
    "# Embedding & Storage\n",
    "\n",
    "from llama_index import ServiceContext, VectorStoreIndex\n",
    "from llama_index.vector_stores import ChromaVectorStore\n",
    "from llama_index.storage.storage_context import StorageContext\n",
    "\n",
    "\n",
    "print(\"Original size collection : \", chroma_collection.count())\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "service_context = ServiceContext.from_defaults(embed_model=embed_model, text_splitter=text_splitter)\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents, storage_context=storage_context, service_context=service_context, show_progress=True\n",
    ")\n",
    "print(\"Post-embeddind size collection : \", chroma_collection.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4 (tags/v3.11.4:d2340ef, Jun  7 2023, 05:45:37) [MSC v.1934 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "0aff181140c6cefaaaf72e346b8c4ff97d18d0699fd8c1d0fe1c867bf0852954"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
